{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ccd267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LAPTOP\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f7bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageDataGenerator(rescale=1/255)\n",
    "validation = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa18d7c",
   "metadata": {},
   "source": [
    "# Morphological Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d3301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dilation and erosion\n",
    "def thin_font(image): # Erosion\n",
    "    image = cv2.bitwise_not(image)\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    image = cv2.erode(image, kernel, iterations=1)\n",
    "    image = cv2.bitwise_not(image)\n",
    "    return image\n",
    "\n",
    "def thick_font(image): # Dilation\n",
    "    image = cv2.bitwise_not(image)\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    image = cv2.dilate(image, kernel, iterations=1)\n",
    "    image = cv2.bitwise_not(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0c3ef",
   "metadata": {},
   "source": [
    "# Edge Detection (Canny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f8ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for edge detection\n",
    "def edge_detection(image):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da99949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_defects = \"./process_training/defects\"\n",
    "training_apples = \"./process_training/apples\"\n",
    "validate_apples = \"./process_validation/apples\"\n",
    "validate_defects = \"./process_validation/defects\"\n",
    "\n",
    "# Output folder to save Edge_Canny images\n",
    "output_train_defects = \"./edge_training/defects\"\n",
    "output_train_apples = \"./edge_training/apples\"\n",
    "output_valid_apples = \"./edge_validation/apples\"\n",
    "output_valid_defects = \"./edge_validation/defects\"\n",
    "\n",
    "\n",
    "# Process images in Training Defects\n",
    "for filename in os.listdir(training_defects):\n",
    "    img_path = os.path.join(training_defects, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "#     thick_img = thick_font(img)           #Current Image passed in has performed morphological operation\n",
    "#     processed_img = thin_font(thick_img)  #Current Image passed in has performed morphological operation\n",
    "    processed_img = edge_detection(img)\n",
    "    output_path = os.path.join(output_train_defects, f'{filename}')\n",
    "    cv2.imwrite(output_path, processed_img)\n",
    "\n",
    "# Process images in Training Apples\n",
    "for filename in os.listdir(training_apples):\n",
    "    img_path = os.path.join(training_apples, filename)\n",
    "#     img = cv2.imread(img_path)  \n",
    "#     thick_img = thick_font(img) \n",
    "    processed_img = edge_detection(img)\n",
    "    output_path = os.path.join(output_train_apples, f'{filename}')\n",
    "    cv2.imwrite(output_path, processed_img)\n",
    "    \n",
    "# Process images in Validate Apples\n",
    "for filename in os.listdir(validate_apples):\n",
    "    img_path = os.path.join(validate_apples, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "#     thick_img = thick_font(img)\n",
    "#     processed_img = thin_font(thick_img)\n",
    "    processed_img = edge_detection(img)\n",
    "    output_path = os.path.join(output_valid_apples, f'{filename}')\n",
    "    cv2.imwrite(output_path, processed_img)\n",
    "    \n",
    "# Process images in Validate Defects\n",
    "for filename in os.listdir(validate_defects):\n",
    "    img_path = os.path.join(validate_defects, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "#     thick_img = thick_font(img)\n",
    "#     processed_img = thin_font(thick_img)\n",
    "    processed_img = edge_detection(img)\n",
    "    output_path = os.path.join(output_valid_defects, f'{filename}')\n",
    "    cv2.imwrite(output_path, processed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "058efff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 456 images belonging to 2 classes.\n",
      "Found 663 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train.flow_from_directory(\n",
    "    './edge_training/',\n",
    "    target_size = (256, 256),\n",
    "    batch_size = 10,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "validation_dataset = validation.flow_from_directory(\n",
    "    './edge_validation/',\n",
    "    target_size = (256, 256),\n",
    "    batch_size = 10,\n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a9a69f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apples': 0, 'defects': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To differentiate between classes\n",
    "train_dataset.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9da35fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(15, (3,3), activation='relu', input_shape=(256,256, 1)),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Conv2D(30, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Conv2D(60, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')                                \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b6f9bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_crossentropy - binary classification (between the true labels and the predicted probabilities) \n",
    "# lr too hight may lead to instability or divergence - overshoot the minimum of the loss\n",
    "# metric - track accuracy (the proportion of correctly classified samples)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer = RMSprop(learning_rate=0.001),\n",
    "              metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1e2e59e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "28/28 [==============================] - 7s 225ms/step - loss: 0.7950 - accuracy: 0.9493 - val_loss: 1.3983 - val_accuracy: 0.2413\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - 6s 215ms/step - loss: 0.0566 - accuracy: 0.9714 - val_loss: 1.8124 - val_accuracy: 0.2413\n",
      "Epoch 3/30\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.5085 - accuracy: 0.9464 - val_loss: 1.4320 - val_accuracy: 0.2413\n",
      "Epoch 4/30\n",
      "28/28 [==============================] - 6s 211ms/step - loss: 0.0505 - accuracy: 0.9964 - val_loss: 1.8115 - val_accuracy: 0.2413\n",
      "Epoch 5/30\n",
      "28/28 [==============================] - 6s 227ms/step - loss: 0.1388 - accuracy: 0.9464 - val_loss: 1.9407 - val_accuracy: 0.2413\n",
      "Epoch 6/30\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 8.1599e-04 - accuracy: 1.0000 - val_loss: 2.2518 - val_accuracy: 0.2413\n",
      "Epoch 7/30\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 2.0675e-04 - accuracy: 1.0000 - val_loss: 2.5883 - val_accuracy: 0.2413\n",
      "Epoch 8/30\n",
      "28/28 [==============================] - 6s 219ms/step - loss: 5.4841e-05 - accuracy: 1.0000 - val_loss: 2.9902 - val_accuracy: 0.2413\n",
      "Epoch 9/30\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.2889 - accuracy: 0.9714 - val_loss: 1.7006 - val_accuracy: 0.2413\n",
      "Epoch 10/30\n",
      "28/28 [==============================] - 6s 211ms/step - loss: 5.0804e-04 - accuracy: 1.0000 - val_loss: 1.9674 - val_accuracy: 0.2413\n",
      "Epoch 11/30\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.1648 - accuracy: 0.9638 - val_loss: 1.5815 - val_accuracy: 0.2413\n",
      "Epoch 12/30\n",
      "28/28 [==============================] - 6s 214ms/step - loss: 4.5655e-04 - accuracy: 1.0000 - val_loss: 1.8487 - val_accuracy: 0.2413\n",
      "Epoch 13/30\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 1.6250e-04 - accuracy: 1.0000 - val_loss: 2.1595 - val_accuracy: 0.2413\n",
      "Epoch 14/30\n",
      "28/28 [==============================] - 6s 221ms/step - loss: 0.1669 - accuracy: 0.9674 - val_loss: 1.4947 - val_accuracy: 0.2413\n",
      "Epoch 15/30\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 8.6567e-04 - accuracy: 1.0000 - val_loss: 1.8845 - val_accuracy: 0.2413\n",
      "Epoch 16/30\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 0.0861 - accuracy: 0.9750 - val_loss: 1.5244 - val_accuracy: 0.2413\n",
      "Epoch 17/30\n",
      "28/28 [==============================] - 6s 215ms/step - loss: 0.0333 - accuracy: 0.9964 - val_loss: 1.7541 - val_accuracy: 0.2413\n",
      "Epoch 18/30\n",
      "28/28 [==============================] - 6s 214ms/step - loss: 6.4459e-04 - accuracy: 1.0000 - val_loss: 2.2004 - val_accuracy: 0.2413\n",
      "Epoch 19/30\n",
      "28/28 [==============================] - 6s 223ms/step - loss: 1.0562e-04 - accuracy: 1.0000 - val_loss: 2.5516 - val_accuracy: 0.2413\n",
      "Epoch 20/30\n",
      "28/28 [==============================] - 6s 221ms/step - loss: 2.4722e-05 - accuracy: 1.0000 - val_loss: 2.8372 - val_accuracy: 0.2413\n",
      "Epoch 21/30\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.1302 - accuracy: 0.9710 - val_loss: 1.7148 - val_accuracy: 0.2413\n",
      "Epoch 22/30\n",
      "28/28 [==============================] - 6s 215ms/step - loss: 0.0349 - accuracy: 0.9964 - val_loss: 1.7036 - val_accuracy: 0.2413\n",
      "Epoch 23/30\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 0.0273 - accuracy: 0.9964 - val_loss: 2.2198 - val_accuracy: 0.2413\n",
      "Epoch 24/30\n",
      "28/28 [==============================] - 6s 225ms/step - loss: 0.0296 - accuracy: 0.9964 - val_loss: 2.4600 - val_accuracy: 0.2413\n",
      "Epoch 25/30\n",
      "28/28 [==============================] - 6s 212ms/step - loss: 0.0347 - accuracy: 0.9964 - val_loss: 2.1924 - val_accuracy: 0.2413\n",
      "Epoch 26/30\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.0280 - accuracy: 0.9964 - val_loss: 2.4930 - val_accuracy: 0.2413\n",
      "Epoch 27/30\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.0324 - accuracy: 0.9964 - val_loss: 2.2071 - val_accuracy: 0.2413\n",
      "Epoch 28/30\n",
      "28/28 [==============================] - 6s 213ms/step - loss: 0.0292 - accuracy: 0.9964 - val_loss: 2.4990 - val_accuracy: 0.2413\n",
      "Epoch 29/30\n",
      "28/28 [==============================] - 6s 220ms/step - loss: 0.0263 - accuracy: 0.9964 - val_loss: 2.9330 - val_accuracy: 0.2413\n",
      "Epoch 30/30\n",
      "28/28 [==============================] - 6s 219ms/step - loss: 2.4334e-04 - accuracy: 1.0000 - val_loss: 3.4220 - val_accuracy: 0.2413\n"
     ]
    }
   ],
   "source": [
    "# steps_per_epoch = how many batches to process in each epoch\n",
    "# lets take batch size as 100\n",
    "# steps_per_epoch = total training data / batch size = 280 / 10 = 28\n",
    "# the model will process 133 steps from the training dataset in each epoch\n",
    "# epochs = train 30 times\n",
    "model_fit = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch = 28,\n",
    "    epochs = 30,\n",
    "    validation_data = validation_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7c16a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./edge_canny_apple.h5') # save performed morphology model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e5ee738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step\n",
      "[[1.]]\n",
      "[1.]\n",
      "1.0\n",
      "This is a defect apple\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = tf.keras.models.load_model('./edge_canny_apple.h5')\n",
    "\n",
    "# Function for edge detection\n",
    "def edge_detection(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (256,256))\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "# Function to preprocess the image for model input\n",
    "def preprocess_img(edges):\n",
    "    # Expand dimensions to match the expected input shape of the model\n",
    "    edges = np.expand_dims(edges, axis=0)\n",
    "    return edges\n",
    "\n",
    "# Function to run prediction\n",
    "def run_prediction(image_path):\n",
    "    # Perform edge detection\n",
    "    edges = edge_detection(image_path)\n",
    "    \n",
    "    # Preprocess the edges\n",
    "    edges = preprocess_img(edges)\n",
    "    \n",
    "    # Make prediction using the loaded model\n",
    "    prediction = loaded_model.predict(edges)\n",
    "    \n",
    "    # Interpret the prediction\n",
    "    print(prediction)\n",
    "    print(prediction[0])\n",
    "    print(prediction[0][0])\n",
    "    if prediction[0][0] == 0:\n",
    "        print(\"This is an apple\")\n",
    "    else:\n",
    "        print(\"This is a defect apple\")\n",
    "\n",
    "# Example usage:\n",
    "image_path = './testData/21.jpg'\n",
    "run_prediction(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "def preprocess_img(edges):\n",
    "    # Expand dimensions to match the expected input shape of the model\n",
    "    edges = np.expand_dims(edges, axis=0)\n",
    "    return edges\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = tf.keras.models.load_model('./edge_canny_apple.h5')\n",
    "\n",
    "# Path to the validation images folder\n",
    "validation_images_folder = \"./edge_validation\"\n",
    "\n",
    "# Initialize lists to store images and labels\n",
    "validation_images = []\n",
    "validation_labels = []\n",
    "\n",
    "# Define labels for the classes\n",
    "class_labels = {\n",
    "    \"apples\": 0,\n",
    "    \"defects\": 1\n",
    "}\n",
    "\n",
    "# Loop through each class folder (apple and defect_apple)\n",
    "for class_name in class_labels.keys():\n",
    "    class_folder = os.path.join(validation_images_folder, class_name)\n",
    "    # Loop through each image in the class folder\n",
    "    for img_file in os.listdir(class_folder):\n",
    "        img_path = os.path.join(class_folder, img_file)\n",
    "        # Read the image and resize it to the desired input shape\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (256, 256))\n",
    "        # Append the image to the validation_images list\n",
    "        validation_images.append(img)\n",
    "        # Append the label to the validation_labels list\n",
    "        validation_labels.append(class_labels[class_name])\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "validation_images = np.array(validation_images)\n",
    "validation_labels = np.array(validation_labels)\n",
    "\n",
    "validation_images = preprocess_img(validation_images)\n",
    "\n",
    "# # Normalize the images (if needed)\n",
    "# validation_images = validation_images.astype('float32') / 255.0\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "validation_loss, validation_accuracy = loaded_model.evaluate(validation_images, validation_labels)\n",
    "\n",
    "# Display the accuracy\n",
    "print(f\"Validation Accuracy: {validation_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
