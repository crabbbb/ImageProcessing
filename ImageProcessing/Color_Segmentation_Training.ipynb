{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f066bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LAPTOP\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a4ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageDataGenerator(rescale=1/255)\n",
    "validation = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89b5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dilation and erosion\n",
    "def thin_font(image): # Erosion\n",
    "    image = cv2.bitwise_not(image)\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    image = cv2.erode(image, kernel, iterations=1)\n",
    "    image = cv2.bitwise_not(image)\n",
    "    return image\n",
    "\n",
    "def thick_font(image): # Dilation\n",
    "    image = cv2.bitwise_not(image)\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    image = cv2.dilate(image, kernel, iterations=1)\n",
    "    image = cv2.bitwise_not(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5701ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for color-based segmentation\n",
    "def color_segmentation(image):\n",
    "    # Convert image to HSV color space\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Define lower and upper bounds for red color\n",
    "    lower_bound = np.array([0, 100, 100])\n",
    "    upper_bound = np.array([10, 255, 255])\n",
    "    \n",
    "    # Threshold the HSV image to get a binary mask\n",
    "    mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "    \n",
    "    # Apply morphological operations to clean up the mask (optional)\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bad73fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_defects = \"./process_training/defects\"\n",
    "training_apples = \"./process_training/apples\"\n",
    "validate_apples = \"./process_validation/apples\"\n",
    "validate_defects = \"./process_validation/defects\"\n",
    "\n",
    "# Output folder to save Color_segmentation images\n",
    "output_train_defects = \"./color_training/defects\"\n",
    "output_train_apples = \"./color_training/apples\"\n",
    "output_valid_apples = \"./color_validation/apples\"\n",
    "output_valid_defects = \"./color_validation/defects\"\n",
    "\n",
    "# Process images in Training Defects\n",
    "for filename in os.listdir(training_defects):\n",
    "    img_path = os.path.join(training_defects, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "#     thick_img = thick_font(img)\n",
    "#     processed_img = thin_font(thick_img)\n",
    "    processed_img = color_segmentation(img)\n",
    "    output_path = os.path.join(output_train_defects, f'{filename}')\n",
    "    cv2.imwrite(output_path, processed_img)\n",
    "\n",
    "# Process images in Training Apples\n",
    "for filename in os.listdir(training_apples):\n",
    "    img_path = os.path.join(training_apples, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "#     thick_img = thick_font(img)\n",
    "#     processed_img = thin_font(thick_img)\n",
    "    processed_img = color_segmentation(img)\n",
    "    output_path = os.path.join(output_train_apples, f'{filename}')\n",
    "    cv2.imwrite(output_path, processed_img)\n",
    "    \n",
    "# Process images in Validate Apples\n",
    "for filename in os.listdir(validate_apples):\n",
    "    img_path = os.path.join(validate_apples, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "#     thick_img = thick_font(img)\n",
    "#     processed_img = thin_font(thick_img)\n",
    "    processed_img = color_segmentation(img)\n",
    "    output_path = os.path.join(output_valid_apples, f'{filename}')\n",
    "    cv2.imwrite(output_path, processed_img)\n",
    "    \n",
    "# Process images in Validate Defects\n",
    "for filename in os.listdir(validate_defects):\n",
    "    img_path = os.path.join(validate_defects, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "#     thick_img = thick_font(img)\n",
    "#     processed_img = thin_font(thick_img)\n",
    "    processed_img = color_segmentation(img)\n",
    "    output_path = os.path.join(output_valid_defects, f'{filename}')\n",
    "    cv2.imwrite(output_path, processed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aaa76ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 456 images belonging to 2 classes.\n",
      "Found 663 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train.flow_from_directory(\n",
    "    './color_training/',\n",
    "    target_size = (256, 256),\n",
    "    batch_size = 10,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "validation_dataset = validation.flow_from_directory(\n",
    "    './color_validation/',\n",
    "    target_size = (256, 256),\n",
    "    batch_size = 10,\n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2153b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apples': 0, 'defects': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To differentiate between classes\n",
    "train_dataset.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fedb54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LAPTOP\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\LAPTOP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(15, (3,3), activation='relu', input_shape=(256,256, 1)),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Conv2D(30, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Conv2D(60, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')                                \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddea551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_crossentropy - binary classification (between the true labels and the predicted probabilities) \n",
    "# lr too hight may lead to instability or divergence - overshoot the minimum of the loss\n",
    "# metric - track accuracy (the proportion of correctly classified samples)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer = RMSprop(learning_rate=0.001),\n",
    "              metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042d8776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\LAPTOP\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\LAPTOP\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "28/28 [==============================] - 21s 659ms/step - loss: 0.7539 - accuracy: 0.6304 - val_loss: 0.7456 - val_accuracy: 0.6516\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - 16s 550ms/step - loss: 0.5030 - accuracy: 0.7571 - val_loss: 0.8788 - val_accuracy: 0.6139\n",
      "Epoch 3/30\n",
      "28/28 [==============================] - 15s 561ms/step - loss: 0.4115 - accuracy: 0.8261 - val_loss: 0.8815 - val_accuracy: 0.6682\n",
      "Epoch 4/30\n",
      "28/28 [==============================] - 15s 538ms/step - loss: 0.2798 - accuracy: 0.8768 - val_loss: 1.0258 - val_accuracy: 0.6802\n",
      "Epoch 5/30\n",
      "28/28 [==============================] - 13s 473ms/step - loss: 0.2159 - accuracy: 0.9143 - val_loss: 1.1348 - val_accuracy: 0.6908\n",
      "Epoch 6/30\n",
      "28/28 [==============================] - 15s 537ms/step - loss: 0.1552 - accuracy: 0.9420 - val_loss: 1.6276 - val_accuracy: 0.7089\n",
      "Epoch 7/30\n",
      "28/28 [==============================] - 15s 558ms/step - loss: 0.0886 - accuracy: 0.9746 - val_loss: 1.8606 - val_accuracy: 0.6637\n",
      "Epoch 8/30\n",
      "28/28 [==============================] - 16s 564ms/step - loss: 0.0800 - accuracy: 0.9710 - val_loss: 1.7572 - val_accuracy: 0.6938\n",
      "Epoch 9/30\n",
      "28/28 [==============================] - 13s 484ms/step - loss: 0.0312 - accuracy: 0.9893 - val_loss: 2.5207 - val_accuracy: 0.6652\n",
      "Epoch 10/30\n",
      "28/28 [==============================] - 15s 532ms/step - loss: 0.0957 - accuracy: 0.9746 - val_loss: 1.9563 - val_accuracy: 0.7210\n",
      "Epoch 11/30\n",
      "28/28 [==============================] - 16s 568ms/step - loss: 0.0155 - accuracy: 0.9964 - val_loss: 2.8828 - val_accuracy: 0.7104\n",
      "Epoch 12/30\n",
      "28/28 [==============================] - 17s 600ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 2.7317 - val_accuracy: 0.6290\n",
      "Epoch 13/30\n",
      "28/28 [==============================] - 14s 487ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.6204 - val_accuracy: 0.7014\n",
      "Epoch 14/30\n",
      "28/28 [==============================] - 15s 531ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.4922 - val_accuracy: 0.7089\n",
      "Epoch 15/30\n",
      "28/28 [==============================] - 15s 547ms/step - loss: 2.5796e-04 - accuracy: 1.0000 - val_loss: 5.0935 - val_accuracy: 0.6968\n",
      "Epoch 16/30\n",
      "28/28 [==============================] - 15s 531ms/step - loss: 7.1431e-05 - accuracy: 1.0000 - val_loss: 5.6501 - val_accuracy: 0.7014\n",
      "Epoch 17/30\n",
      "28/28 [==============================] - 13s 470ms/step - loss: 2.5178e-05 - accuracy: 1.0000 - val_loss: 6.0405 - val_accuracy: 0.7029\n",
      "Epoch 18/30\n",
      "28/28 [==============================] - 14s 509ms/step - loss: 3.7714e-05 - accuracy: 1.0000 - val_loss: 6.0361 - val_accuracy: 0.6953\n",
      "Epoch 19/30\n",
      "28/28 [==============================] - 15s 560ms/step - loss: 2.1603e-05 - accuracy: 1.0000 - val_loss: 6.4351 - val_accuracy: 0.6998\n",
      "Epoch 20/30\n",
      "28/28 [==============================] - 15s 555ms/step - loss: 1.4491e-05 - accuracy: 1.0000 - val_loss: 6.5182 - val_accuracy: 0.6998\n",
      "Epoch 21/30\n",
      "28/28 [==============================] - 13s 485ms/step - loss: 1.4810e-05 - accuracy: 1.0000 - val_loss: 6.7605 - val_accuracy: 0.7029\n",
      "Epoch 22/30\n",
      "28/28 [==============================] - 14s 491ms/step - loss: 1.0458e-05 - accuracy: 1.0000 - val_loss: 6.8375 - val_accuracy: 0.7014\n",
      "Epoch 23/30\n",
      "28/28 [==============================] - 15s 545ms/step - loss: 8.0574e-06 - accuracy: 1.0000 - val_loss: 6.8908 - val_accuracy: 0.7014\n",
      "Epoch 24/30\n",
      "28/28 [==============================] - 15s 556ms/step - loss: 1.6981e-06 - accuracy: 1.0000 - val_loss: 6.9628 - val_accuracy: 0.7014\n",
      "Epoch 25/30\n",
      "28/28 [==============================] - 14s 509ms/step - loss: 2.3629e-06 - accuracy: 1.0000 - val_loss: 7.0145 - val_accuracy: 0.7014\n",
      "Epoch 26/30\n",
      "28/28 [==============================] - 13s 464ms/step - loss: 6.6377e-06 - accuracy: 1.0000 - val_loss: 6.9668 - val_accuracy: 0.7014\n",
      "Epoch 27/30\n",
      "28/28 [==============================] - 15s 522ms/step - loss: 1.3219e-06 - accuracy: 1.0000 - val_loss: 7.0696 - val_accuracy: 0.7029\n",
      "Epoch 28/30\n",
      "28/28 [==============================] - 15s 560ms/step - loss: 5.2247e-06 - accuracy: 1.0000 - val_loss: 7.1769 - val_accuracy: 0.7029\n",
      "Epoch 29/30\n",
      "28/28 [==============================] - 15s 535ms/step - loss: 5.0635e-06 - accuracy: 1.0000 - val_loss: 7.2440 - val_accuracy: 0.7029\n",
      "Epoch 30/30\n",
      "28/28 [==============================] - 12s 418ms/step - loss: 9.5869e-07 - accuracy: 1.0000 - val_loss: 7.3591 - val_accuracy: 0.7044\n"
     ]
    }
   ],
   "source": [
    "# steps_per_epoch = how many batches to process in each epoch\n",
    "# lets take batch size as 100\n",
    "# steps_per_epoch = total training data / batch size = 280 / 10 = 28\n",
    "# the model will process 133 steps from the training dataset in each epoch\n",
    "# epochs = train 30 times\n",
    "model_fit = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch = 28,\n",
    "    epochs = 30,\n",
    "    validation_data = validation_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f569a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./color_segmentation_apple.h5') # save performed morphology model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d0189bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 156ms/step\n",
      "[[1.]]\n",
      "[1.]\n",
      "1.0\n",
      "This is a defect apple\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = tf.keras.models.load_model('./color_segmentation_apple.h5')\n",
    "\n",
    "# Function to preprocess the image for model input\n",
    "def preprocess_img(edges):\n",
    "    # Expand dimensions to match the expected input shape of the model\n",
    "    color = np.expand_dims(edges, axis=0)\n",
    "    return color\n",
    "\n",
    "# Function for color-based segmentation\n",
    "def color_segmentation(image):\n",
    "    # Convert image to HSV color space\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hsv = cv2.resize(hsv, (256,256))\n",
    "    \n",
    "    # Define lower and upper bounds for red color\n",
    "    lower_bound = np.array([0, 100, 100])\n",
    "    upper_bound = np.array([10, 255, 255])\n",
    "    \n",
    "    # Threshold the HSV image to get a binary mask\n",
    "    mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "    \n",
    "    # Apply morphological operations to clean up the mask (optional)\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Function to run prediction\n",
    "def run_prediction(image_path):\n",
    "    # Perform edge detection\n",
    "    img = cv2.imread(image_path)\n",
    "    images = color_segmentation(img)\n",
    "    \n",
    "    images = preprocess_img(images)\n",
    "    \n",
    "    # Make prediction using the loaded model\n",
    "    prediction = loaded_model.predict(images)\n",
    "    \n",
    "    # Interpret the prediction\n",
    "    print(prediction)\n",
    "    print(prediction[0])\n",
    "    print(prediction[0][0])\n",
    "    if prediction[0][0] == 0:\n",
    "        print(\"This is an apple\")\n",
    "    else:\n",
    "        print(\"This is a defect apple\")\n",
    "\n",
    "# Example usage:\n",
    "image_path = './testData/21.jpg'\n",
    "run_prediction(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc9daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
